# -*- coding: utf-8 -*-
"""Is4200-ground_truthLabeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uA7LiJeNa7m3MwzUjhnJ8muTlS9JlUGj
"""

# Commented out IPython magic to ensure Python compatibility.
token = "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"
username = "p27sinha"

!git clone https://{username}:{token}@github.com/agasti-mhatre/CS6200-Project.git
# %cd CS6200-Project

!git checkout -b stats_for_ground_truth

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!ls "/content/drive/MyDrive/IS4200_Final_Project/business_with_reviews.json"

import json
from collections import Counter

input_path = "/content/drive/MyDrive/IS4200_Final_Project/yelp_academic_dataset_business.json"
output_path = "/content/drive/MyDrive/IS4200_Final_Project/small_business_sample.json"

sample_size = 5000
city_counter = Counter()
sampled_businesses = []

with open(input_path, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i >= sample_size:
            break
        business = json.loads(line)
        sampled_businesses.append(business)
        city = (business.get("city") or "").strip().lower()
        if city:
            city_counter[city] += 1

# Write sampled businesses to a new JSONL file
with open(output_path, "w", encoding="utf-8") as f_out:
    for biz in sampled_businesses:
        json.dump(biz, f_out)
        f_out.write("\n")

# Print most common cities
print("Top 10 Cities in Sample:")
for city, count in city_counter.most_common(10):
    print(f"{city.title()}: {count}")

import json

review_path = "/content/drive/MyDrive/IS4200_Final_Project/business_with_reviews.json"

with open(review_path, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i >= 5:
            break
        try:
            review = json.loads(line)
            print(review)
        except json.JSONDecodeError:
            print(f"Skipping malformed line {i}")

query = "Cafe with great lattes in Philadelphia"
query_keywords = ["latte", "coffee", "espresso"]
query_location = "philadelphia"

import json
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# --- Metadata-Only Labeling Function ---
def label_from_metadata_only(business_info, query_keywords, query_location, min_stars=3.5):
    score = 0

    categories = (business_info.get("categories") or "").lower()
    name = (business_info.get("name") or "").lower()
    city = (business_info.get("city") or "").lower().strip()
    stars = float(business_info.get("stars", 0))

    matched_tokens = any(kw in categories or kw in name for kw in query_keywords)
    if matched_tokens:
        score += 1
    if city == query_location.lower().strip():
        score += 1
    if stars >= min_stars:
        score += 1

    if score == 3:
        return "highly"
    elif score == 2:
        return "somewhat"
    else:
        return "irrelevant"

# --- Ground Truth Builder + Review Ranking ---
def build_ground_truth_metadata_with_rerank(
    business_path,
    review_path,
    query,
    query_keywords,
    location,
    max_matches=5000,
    min_rating=3.5
):
    business_metadata = {}
    with open(business_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            business_metadata[data["business_id"]] = {
                "name": data.get("name", ""),
                "city": data.get("city", ""),
                "stars": data.get("stars", 0),
                "categories": data.get("categories", "")
            }

    ground_truth = {}
    ranked_candidates = {}
    review_texts = {}

    with open(review_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                r = json.loads(line)
                bid = r.get("business_id")
                if bid in business_metadata:
                    review_texts.setdefault(bid, []).append(r.get("text", ""))
            except:
                continue

    count = 0
    for bid, info in business_metadata.items():
        label = label_from_metadata_only(
            business_info=info,
            query_keywords=query_keywords,
            query_location=location,
            min_stars=min_rating
        )
        ground_truth[bid] = label
        if label in ["somewhat", "highly"] and bid in review_texts:
            ranked_candidates[bid] = " ".join(review_texts[bid])
        count += 1
        if count >= max_matches:
            break

    # --- TF-IDF reranking ---
    vectorizer = TfidfVectorizer()
    corpus = list(ranked_candidates.values())
    biz_ids = list(ranked_candidates.keys())
    tfidf_matrix = vectorizer.fit_transform(corpus)
    query_vec = vectorizer.transform([query])
    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()

    reranked = sorted(zip(biz_ids, scores), key=lambda x: x[1], reverse=True)
    top_ranked = reranked[:10]

    print("\n=== TOP HIGHLY RELEVANT BUSINESSES (RERANKED BY REVIEW MATCHING) ===")
    for i, (bid, score) in enumerate(top_ranked):
        info = business_metadata[bid]
        if ground_truth[bid] == "highly":
            print(f"\nRank {i+1} | Score: {score:.4f}")
            print(f"Name: {info['name']}")
            print(f"City: {info['city']}")
            print(f"Stars: {info['stars']}")
            print(f"Categories: {info['categories']}")

    return {query: ground_truth}, business_metadata

# --- Visualization Function ---
def plot_label_distribution(ground_truth, query):
    labels = ground_truth[query]
    label_counts = Counter(labels.values())
    plt.bar(label_counts.keys(), label_counts.values(), color=["green", "orange", "red"])
    plt.title(f"Label Distribution for: {query}")
    plt.xlabel("Relevance Label")
    plt.ylabel("Number of Businesses")
    plt.grid(axis="y")
    plt.tight_layout()
    plt.show()
    print("Label counts:", label_counts)

# --- Entry Point ---
if __name__ == "__main__":
    business_path = "/content/drive/MyDrive/IS4200_Final_Project/small_business_sample.json"
    review_path = "/content/drive/MyDrive/IS4200_Final_Project/yelp_academic_dataset_review.json"
    query = "Cafe with great lattes in Philadelphia"
    query_keywords = ["latte", "coffee", "espresso"]
    location = "philadelphia"

    ground_truth, business_metadata = build_ground_truth_metadata_with_rerank(
        business_path=business_path,
        review_path=review_path,
        query=query,
        query_keywords=query_keywords,
        location=location,
        max_matches=5000,
        min_rating=3.5
    )

    with open("ground_truth_final.json", "w") as f:
        json.dump(ground_truth, f, indent=2)

    plot_label_distribution(ground_truth, query)

import json

review_path = "/content/drive/MyDrive/IS4200_Final_Project/yelp_academic_dataset_review.json"

with open(review_path, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i >= 5:
            break
        try:
            review = json.loads(line)
            print(review)
        except json.JSONDecodeError:
            print(f"Skipping malformed line {i}")

def build_ground_truth_metadata_with_rerank_highly(
    business_path,
    review_path,
    query,
    query_keywords,
    location,
    max_matches=5000,
    min_rating=3.5
):
    business_metadata = {}
    with open(business_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            business_metadata[data["business_id"]] = {
                "name": data.get("name", ""),
                "city": data.get("city", ""),
                "stars": data.get("stars", 0),
                "categories": data.get("categories", "")
            }

    ground_truth = {}
    ranked_candidates = {}
    review_texts = {}

    with open(review_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                r = json.loads(line)
                bid = r.get("business_id")
                if bid in business_metadata:
                    review_texts.setdefault(bid, []).append(r.get("text", ""))
            except:
                continue

    count = 0
    for bid, info in business_metadata.items():
        label = label_from_metadata_only(
            business_info=info,
            query_keywords=query_keywords,
            query_location=location,
            min_stars=min_rating
        )
        ground_truth[bid] = label
        if label in ["somewhat", "highly"] and bid in review_texts:
            ranked_candidates[bid] = " ".join(review_texts[bid])
        count += 1
        if count >= max_matches:
            break

    # Step 4: Rerank ONLY businesses labeled as "highly" using their reviews
    highly_biz_ids = [bid for bid in ground_truth if ground_truth[bid] == "highly" and bid in review_texts]
    highly_corpus = [" ".join(review_texts[bid]) for bid in highly_biz_ids]

    # TF-IDF vectorization
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(highly_corpus)
    query_vec = vectorizer.transform([query])
    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()

    # Combine business ID with similarity scores and sort
    reranked_highly = sorted(zip(highly_biz_ids, scores), key=lambda x: x[1], reverse=True)

    # Output ranked list
    print("\n=== ALL HIGHLY RELEVANT BUSINESSES (RANKED BY TF-IDF REVIEW SIMILARITY) ===")
    for i, (bid, score) in enumerate(reranked_highly):
        info = business_metadata[bid]
        print(f"\nRank {i+1} | Score: {score:.4f}")
        print(f"Name: {info['name']}")
        print(f"City: {info['city']}")
        print(f"Stars: {info['stars']}")
        print(f"Categories: {info['categories']}")

    return ground_truth, business_metadata

if __name__ == "__main__":
    business_path = "/content/drive/MyDrive/IS4200_Final_Project/small_business_sample.json"
    review_path = "/content/drive/MyDrive/IS4200_Final_Project/yelp_academic_dataset_review.json"
    query = "Cafe with great lattes in Philadelphia"
    query_keywords = ["latte", "coffee", "espresso"]
    location = "philadelphia"

    ground_truth, business_metadata = build_ground_truth_metadata_with_rerank_highly(
        business_path=business_path,
        review_path=review_path,
        query=query,
        query_keywords=query_keywords,
        location=location,
        max_matches=5000,
        min_rating=3.5
    )

import json
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Optional import for A+
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None


# -------------------------------------------------------------------
# Level B: Basic token match in reviews + metadata filters
# -------------------------------------------------------------------
def build_ground_truth_level_B(
    business_path,
    review_path,
    query_keywords,
    location,
    max_matches=5000,
    min_stars=0.0
):
    """B: Label 'somewhat' if any query keyword appears in reviews,
       after filtering by city and café category."""
    # load metadata
    biz_meta = {}
    with open(business_path) as f:
        for i, line in enumerate(f):
            if i >= max_matches: break
            biz = json.loads(line)
            biz_meta[biz["business_id"]] = biz

    # load reviews (up to 10 per biz)
    reviews = defaultdict(list)
    with open(review_path) as f:
        for line in f:
            r = json.loads(line)
            bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid]) < 10:
                reviews[bid].append(r["text"].lower())

    # label
    gt = {}
    for bid, info in biz_meta.items():
        city = info.get("city","").lower().strip()
        cats = (info.get("categories") or "").lower()
        stars = float(info.get("stars",0))
        # metadata filter
        if city != location or stars < min_stars or not any(k in cats for k in ["cafe","coffee","espresso","tea"]):
            gt[bid] = "irrelevant"
            continue
        text = " ".join(reviews.get(bid, []))
        gt[bid] = "somewhat" if any(qk in text for qk in query_keywords) else "irrelevant"
    return gt, biz_meta


# -------------------------------------------------------------------
# Level B+: Add rating threshold
# -------------------------------------------------------------------
def build_ground_truth_level_B_plus(
    business_path,
    review_path,
    query_keywords,
    location,
    max_matches=5000,
    min_stars=3.0
):
    """B+: Same as B, but enforce a star‐rating threshold."""
    return build_ground_truth_level_B(
        business_path, review_path, query_keywords, location,
        max_matches=max_matches, min_stars=min_stars
    )


# -------------------------------------------------------------------
# Level A-: TF‑IDF on reviews + metadata & rating filters
# -------------------------------------------------------------------
def build_ground_truth_level_A_minus(
    business_path,
    review_path,
    query,
    query_keywords,
    location,
    max_matches=5000,
    min_stars=3.0,
    thresholds=(0.2, 0.08)
):
    """A‑: Use TF‑IDF similarity to promote 'highly'."""
    # first build B+ ground truth and load its reviews
    gt_Bp, biz_meta = build_ground_truth_level_B_plus(
        business_path, review_path, query_keywords, location,
        max_matches=max_matches, min_stars=min_stars
    )
    # collect reviews for those labeled somewhat/highly
    reviews = defaultdict(list)
    with open(review_path) as f:
        for line in f:
            r = json.loads(line); bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid])<10:
                reviews[bid].append(r["text"].lower())
    # fit TF‑IDF on all candidate reviews
    corpus = [" ".join(reviews[bid]) for bid,label in gt_Bp.items() if label!="irrelevant"]
    vectorizer = TfidfVectorizer().fit(corpus)
    q_vec = vectorizer.transform([query])
    # reassign labels
    gt = {}
    for bid, base in gt_Bp.items():
        if base=="irrelevant":
            gt[bid] = "irrelevant"
            continue
        doc_vec = vectorizer.transform([" ".join(reviews[bid])])
        sim = cosine_similarity(q_vec, doc_vec).flatten()[0]
        if sim >= thresholds[0]:
            gt[bid] = "highly"
        elif sim >= thresholds[1]:
            gt[bid] = "somewhat"
        else:
            gt[bid] = "irrelevant"
    return gt, biz_meta


# -------------------------------------------------------------------
# Level A+: SBERT embeddings + all previous filters
# -------------------------------------------------------------------
def build_ground_truth_level_A_plus(
    business_path,
    review_path,
    query,
    query_keywords,
    location,
    max_matches=5000,
    min_stars=3.0,
    thresholds=(0.5,0.3)
):
    """A+: Use SBERT to compute semantic similarity."""
    if SentenceTransformer is None:
        raise ImportError("Please pip install sentence-transformers to use A+.")
    # reuse B+ metadata filter
    gt_Bp, biz_meta = build_ground_truth_level_B_plus(
        business_path, review_path, query_keywords, location,
        max_matches=max_matches, min_stars=min_stars
    )
    # load reviews
    reviews = defaultdict(list)
    with open(review_path) as f:
        for line in f:
            r = json.loads(line); bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid])<10:
                reviews[bid].append(r["text"])
    # prepare SBERT
    model = SentenceTransformer("all-MiniLM-L6-v2")
    q_emb = model.encode(query)
    # label
    gt = {}
    for bid, base in gt_Bp.items():
        if base=="irrelevant":
            gt[bid] = "irrelevant"; continue
        text = " ".join(reviews[bid])
        doc_emb = model.encode(text)
        sim = cosine_similarity([q_emb],[doc_emb]).flatten()[0]
        if sim >= thresholds[0]:
            gt[bid] = "highly"
        elif sim >= thresholds[1]:
            gt[bid] = "somewhat"
        else:
            gt[bid] = "irrelevant"
    return gt, biz_meta


# -------------------------------------------------------------------
# Example usage:
# -------------------------------------------------------------------
if __name__ == "__main__":
    bp = "/content/drive/MyDrive/IS4200_Final_Project/small_business_sample.json"
    rp = "/content/drive/MyDrive/IS4200_Final_Project/yelp_academic_dataset_review.json"
    qry = "Cafe with great lattes in Philadelphia"
    kws = ["latte","coffee","espresso"]
    loc = "philadelphia"

    # pick the desired level:
    gtB, metaB = build_ground_truth_level_B(bp,rp,kws,loc)
    gtBp, _    = build_ground_truth_level_B_plus(bp,rp,kws,loc)
    gtA, metaA = build_ground_truth_level_A_minus(bp,rp, qry,kws,loc)
    gtAp, _    = build_ground_truth_level_A_plus(bp,rp, qry,kws,loc)

    # now gtB, gtBp, gtA, gtAp map biz_id -> {irrelevant,somewhat,highly}

import json
from itertools import islice
from collections import Counter
from collections import defaultdict

if __name__=="__main__":
    bp  = "/content/drive/MyDrive/IS4200_Final_Project/small_business_sample.json"
    rp  = "/content/drive/MyDrive/IS4200_Final_Project/yelp_academic_dataset_review.json"
    qry = "Cafe with great lattes in Philadelphia"
    kws = ["latte","coffee","espresso"]
    loc = "philadelphia"

    # Build all four ground truths
    gtB, metaB   = build_ground_truth_level_B(     bp, rp, kws, loc)
    gtBp, _      = build_ground_truth_level_B_plus(bp, rp, kws, loc)
    gtA, metaA   = build_ground_truth_level_A_minus(bp, rp, qry, kws, loc)
    gtAp, metaAp = build_ground_truth_level_A_plus( bp, rp, qry, kws, loc)

    def sample_per_label(gt, meta, per_label=1):
      """
      Return up to `per_label` examples for each label in gt.
      """
      buckets = defaultdict(list)
      for bid, label in gt.items():
          if len(buckets[label]) < per_label:
              buckets[label].append(bid)
          # once every label has enough, we can stop
          if all(len(v) >= per_label for v in buckets.values()):
              break

      # flatten to (name, label) pairs
      samples = []
      for label, bids in buckets.items():
          for bid in bids:
              samples.append((meta[bid]["name"], label))
      return samples

    # Helper to print summary + sample
    def report(gt, meta, level):
        print(f"\n--- Level {level} ---")
        counts = Counter(gt.values())
        print("Counts:", counts)

        # get up to 1 example per label
        samples = sample_per_label(gt, meta, per_label=1)
        print("Sample:")
        for name, label in samples:
          print(f"  {name[:30]:30s} → {label}")

    report(gtB,   metaB,  "B")
    report(gtBp,  metaB,  "B+")
    report(gtA,   metaA,  "A-")
    report(gtAp,  metaAp, "A+")

    # (Optional) save one of them to disk
    # with open("ground_truth_A_minus.json", "w") as f:
        # json.dump({qry: gtA}, f, indent=2)

"""
is4200_ground_truthlabeling.py

Generates ground-truth labels for four grading tiers (B, B+, A-, A+),
exports JSON files, and visualizes label distributions.
"""

import json
from collections import defaultdict, Counter
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Optional SBERT import for A+
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None


# ----------------------------
# Level B: Metadata + token match
# ----------------------------
def build_ground_truth_level_B(biz_path, rev_path, query_keywords, location,
                               max_matches=5000, min_stars=0.0):
    biz_meta = {}
    with open(biz_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= max_matches: break
            b = json.loads(line)
            biz_meta[b["business_id"]] = b

    # load up to 10 reviews per business
    reviews = defaultdict(list)
    with open(rev_path, "r", encoding="utf-8") as f:
        for line in f:
            r = json.loads(line)
            bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid]) < 10:
                reviews[bid].append(r["text"].lower())

    gt = {}
    for bid, info in biz_meta.items():
        city = info.get("city","").lower().strip()
        cats = (info.get("categories") or "").lower()
        stars = float(info.get("stars",0))
        # metadata filters
        if city != location or stars < min_stars or not any(k in cats for k in ["cafe","coffee","espresso","tea"]):
            gt[bid] = "irrelevant"
        else:
            text = " ".join(reviews.get(bid, []))
            gt[bid] = "somewhat" if any(qk in text for qk in query_keywords) else "irrelevant"
    return gt, biz_meta


# ----------------------------
# Level B+: add rating threshold
# ----------------------------
def build_ground_truth_level_B_plus(biz_path, rev_path, query_keywords, location,
                                    max_matches=5000, min_stars=3.0):
    return build_ground_truth_level_B(biz_path, rev_path, query_keywords, location,
                                     max_matches=max_matches, min_stars=min_stars)


# ----------------------------
# Level A-: TF‑IDF similarity
# ----------------------------
def build_ground_truth_level_A_minus(biz_path, rev_path, query, query_keywords,
                                     location, max_matches=5000, min_stars=3.0,
                                     thresholds=(0.2, 0.08)):
    # start from B+
    gtB, biz_meta = build_ground_truth_level_B_plus(
        biz_path, rev_path, query_keywords, location,
        max_matches=max_matches, min_stars=min_stars
    )
    # collect reviews for candidates
    reviews = defaultdict(list)
    with open(rev_path, "r", encoding="utf-8") as f:
        for line in f:
            r = json.loads(line)
            bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid]) < 10:
                reviews[bid].append(r["text"].lower())

    # fit TF‑IDF on all non-irrelevant review docs
    corpus = [" ".join(reviews[bid]) for bid,label in gtB.items() if label!="irrelevant"]
    vectorizer = TfidfVectorizer().fit(corpus)
    q_vec = vectorizer.transform([query])

    gt = {}
    for bid, base in gtB.items():
        if base == "irrelevant":
            gt[bid] = "irrelevant"; continue
        doc_vec = vectorizer.transform([" ".join(reviews[bid])])
        sim = cosine_similarity(q_vec, doc_vec).flatten()[0]
        if sim >= thresholds[0]:
            gt[bid] = "highly"
        elif sim >= thresholds[1]:
            gt[bid] = "somewhat"
        else:
            gt[bid] = "irrelevant"
    return gt, biz_meta


# ----------------------------
# Level A+: SBERT similarity
# ----------------------------
def build_ground_truth_level_A_plus(biz_path, rev_path, query, query_keywords,
                                    location, max_matches=5000, min_stars=3.0,
                                    thresholds=(0.5, 0.3)):
    if SentenceTransformer is None:
        raise ImportError("Install sentence-transformers to use A+")
    # start from B+
    gtB, biz_meta = build_ground_truth_level_B_plus(
        biz_path, rev_path, query_keywords, location,
        max_matches=max_matches, min_stars=min_stars
    )
    # collect reviews
    reviews = defaultdict(list)
    with open(rev_path, "r", encoding="utf-8") as f:
        for line in f:
            r = json.loads(line)
            bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid]) < 10:
                reviews[bid].append(r["text"])

    model = SentenceTransformer("all-MiniLM-L6-v2")
    q_emb = model.encode(query)

    gt = {}
    for bid, base in gtB.items():
        if base == "irrelevant":
            gt[bid] = "irrelevant"; continue
        text = " ".join(reviews[bid])
        doc_emb = model.encode(text)
        sim = cosine_similarity([q_emb], [doc_emb]).flatten()[0]
        if sim >= thresholds[0]:
            gt[bid] = "highly"
        elif sim >= thresholds[1]:
            gt[bid] = "somewhat"
        else:
            gt[bid] = "irrelevant"
    return gt, biz_meta


# ----------------------------
# Utility: plot distribution
# ----------------------------
def plot_label_distribution(gt, level, query):
    counts = Counter(gt.values())
    plt.figure(figsize=(5,3))
    plt.bar(counts.keys(), counts.values(), color=["green","orange","red"])
    plt.title(f"Level {level} GT Distribution\n\"{query}\"")
    plt.xlabel("Label"); plt.ylabel("Count")
    plt.grid(axis="y"); plt.tight_layout()
    plt.show()
    print("Counts:", counts)


# ----------------------------
# Utility: print sample per label
# ----------------------------
def sample_per_label(gt, meta, per_label=1):
    buckets = defaultdict(list)
    for bid, label in gt.items():
        if len(buckets[label]) < per_label:
            buckets[label].append(bid)
        if all(len(v)>=per_label for v in buckets.values()):
            break
    for label, bids in buckets.items():
        for bid in bids:
            name = meta[bid]["name"]
            print(f"  {name[:30]:30s} → {label}")
    if not buckets:
        print("  (no labels)")


# ----------------------------
# Main driver
# ----------------------------
if __name__ == "__main__":
    import matplotlib.pyplot as plt

    biz_path = "/content/drive/MyDrive/IS4200_Final_Project/small_business_sample.json"
    rev_path = "/content/drive/MyDrive/IS4200_Final_Project/yelp_academic_dataset_review.json"
    query    = "Cafe with great lattes in Philadelphia"
    keywords = ["latte","coffee","espresso"]
    location = "philadelphia"

    # run each level
    for level, builder in [
        ("B",   lambda: build_ground_truth_level_B(biz_path, rev_path, keywords, location)),
        ("B+",  lambda: build_ground_truth_level_B_plus(biz_path, rev_path, keywords, location)),
        ("A-",  lambda: build_ground_truth_level_A_minus(biz_path, rev_path, query, keywords, location)),
        ("A+",  lambda: build_ground_truth_level_A_plus(biz_path, rev_path, query, keywords, location)),
    ]:
        print(f"\n=== Ground Truth Level {level} ===")
        gt, meta = builder()
        # plot
        plot_label_distribution(gt, level, query)
        # sample one of each label
        print("Sample per label:")
        sample_per_label(gt, meta, per_label=1)
        # save to JSON
        out_file = f"ground_truth_level_{level.replace('+','plus')}.json"
        with open(out_file, "w") as f:
            json.dump({query: gt}, f, indent=2)
        print(f"→ saved {out_file}")