# -*- coding: utf-8 -*-
"""yelp_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v31cbjdBg2hvtqY59TBByeHSer6Rw2Kn
"""

# Import necessary libraries

import json
import joblib
from collections import defaultdict
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import pandas as pd
import scipy.sparse
from math import radians, cos, sin, asin, sqrt
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import math 

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

"""## Step 1: Join business dataset with reviews dataset. Tokenize reviews and convert to lowercase"""

# --- CONFIG ---
MAX_REVIEWS = 10000  # You can increase this later
PRINT_EVERY = 1000

# Step 1: Load Reviews and Track Which Businesses Have Them
print("Loading reviews...")
raw_reviews = defaultdict(list)
with open("yelp_academic_dataset_review.json", "r", encoding="utf-8") as file:
    for i, line in enumerate(file):
        if i >= MAX_REVIEWS:
            break
        if i % PRINT_EVERY == 0:
            print(f"Loaded {i} reviews")
        review = json.loads(line)
        raw_reviews[review["business_id"]].append(review["text"])

# Step 2: Load ONLY Businesses That Have Reviews
print("Loading matching businesses...")
business_information = defaultdict(lambda: {"reviews": []})
with open("yelp_academic_dataset_business.json", "r", encoding="utf-8") as file:
    for line in file:
        business = json.loads(line)
        bid = business["business_id"]
        if bid in raw_reviews:
            business_information[bid].update({
                "name": business["name"],
                "address": business["address"],
                "city": business["city"],
                "state": business["state"],
                "latitude": business["latitude"],
                "longitude": business["longitude"],
                "stars": business["stars"],
                "reviews": raw_reviews[bid]  # add reviews directly
            })

print(f"Loaded {len(business_information)} businesses with reviews.")

# Step 3: Tokenize Reviews
tokenized_business_information = defaultdict(lambda: {"reviews": []})
print("Tokenizing reviews...")
for i, (bid, info) in enumerate(business_information.items()):
    if i % PRINT_EVERY == 0:
        print(f"Tokenizing business {i}")
    for review in info["reviews"]:
        tokens = word_tokenize(review.lower())
        tokenized_business_information[bid]["reviews"].append(tokens)

# Step 4: Save to Output
print("Saving tokenized businesses with reviews...")
with open("business_with_reviews.json", "w") as f:
    for key, value in tokenized_business_information.items():
        json.dump({key: value}, f)
        f.write('\n')

print("Done!")


# Extract cities from your business subset
city_counts = Counter()

for info in business_information.values():
    city = info.get("city", "").strip()
    if city:
        city_counts[city] += 1

# Get top 10 most common cities
top_10_cities = city_counts.most_common(10)

# Print results
print("Top 10 Cities in Subset:")
for city, count in top_10_cities:
    print(f"{city}: {count} businesses")

def get_map_100(ranked_list, ground_truth, relevant_label="highly"):
    relevant_found = 0
    precision_total = 0.0

    for i, (bid, _) in enumerate(ranked_list[:100]):
        if ground_truth.get(bid) == relevant_label:
            relevant_found += 1
            precision_total += relevant_found / (i + 1)

    return precision_total / relevant_found if relevant_found > 0 else 0.0

query = "live music restaurants in Nashville"
keywords = ["music", "live", "bar", "band"]
location = "nashville"

def dcg_at_k(ranked_list, ground_truth, k=100):
    dcg = 0.0
    for i, (bid, _) in enumerate(ranked_list[:k]):
        rel = 0
        if ground_truth.get(bid) == "highly":
            rel = 2
        elif ground_truth.get(bid) == "somewhat":
            rel = 1
        # Use log base 2 for discount
        dcg += (2**rel - 1) / math.log2(i + 2)
    return dcg

def ndcg_at_k(ranked_list, ground_truth, k=100):
    ideal_list = sorted(
        [(bid, 2 if val == "highly" else 1 if val == "somewhat" else 0)
         for bid, val in ground_truth.items()],
        key=lambda x: x[1], reverse=True
    )[:k]
    ideal_dcg = sum((2**rel - 1) / math.log2(i + 2) for i, (_, rel) in enumerate(ideal_list))
    actual_dcg = dcg_at_k(ranked_list, ground_truth, k)
    return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0.0



"""## Step 2: Output scores for TFIDF ranker"""
print("STEP 2")
# Read in business reviews from JSON file
business_reviews = dict()

with open('business_with_reviews.json', 'r') as f:

    for line in f:

        record = json.loads(line)
        business_reviews.update(record)

# Test one query on the entire dataset, check results
business_ids = list(business_reviews.keys())
corpus = []
for id_ in business_ids:

    # Each business review is condensed down into a single, long sentence
    # so that TFIDF and other embedding techniques could be applied
    corpus.append(" ".join(" ".join(tokens) for tokens in business_reviews[id_]["reviews"]))

# Create TFIDF vector and train vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(corpus)

# Persist vector and model, and ids
joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')
scipy.sparse.save_npz("tfidf_matrix.npz", tfidf_matrix)
with open("business_ids.json", "w") as f: json.dump(business_ids, f)

# Reinitialize vectorizer and tfidf_matrix if session is lost
vectorizer = joblib.load('tfidf_vectorizer.joblib')
tfidf_matrix = scipy.sparse.load_npz("tfidf_matrix.npz")
with open("business_ids.json") as f: business_ids = json.load(f)

# Transform query so that it fits into TFIDF vector space
query = "live music restaurants in Nashville"
query_vec = vectorizer.transform([query])

# Use cosine similarity to find most similar document for the query
similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()

# Match up business ids with similarity scores
scores = []

n = len(business_ids)
for i in range(n): scores.append((business_ids[i], similarities[i]))

# Sort based on score; sort in descending order
scores.sort(key = lambda x: x[1], reverse=True)

# Print out top 10 results
for id, _ in scores[:10]:

    print(business_information[id]["name"])

"""## Step 3: Create/Store Embeddings and Use FAISS to generate scores, Compare to token-matching ranker"""
print("STEP 3")
# Use transformer architecture to create dense vector embeddings
# from query and restaurant reviews
# all = general purpose transformer
# MiniLM = lightweight transformer similar to BERT
# L6 = six layers
# v2 = version two
model = SentenceTransformer("all-MiniLM-L6-v2")

# Create embeddings from the reviews
embeddings = model.encode(corpus, convert_to_numpy=True)
dimension = embeddings.shape[1]

# Create FAISS index for similarity search
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

# Persist embeddings to file
np.savez("transformer_embeddings.npz", ids=business_ids, vectors=embeddings)

# Read embeddings back into matrix if session is lost

data = np.load("transformer_embeddings.npz", allow_pickle=True)
business_ids = data["ids"].tolist()
embeddings = data["vectors"]

# Persist index to file
faiss.write_index(index, "business_index.faiss")

# Read index back from file if session is lost
index = faiss.read_index("business_index.faiss")

# Check results of test query
query = "live music restaurants in Nashville"
query_vec = model.encode([query], convert_to_numpy=True)

# Get top k results
k = 50
D, I = index.search(query_vec, k)
top_businesses = [business_ids[i] for i in I[0]]

for id in top_businesses:

    print(business_information[id]["name"])

"""## Batuhan's part"""
print("BATUHAN")
bordering_states = {
    'AL': ['FL', 'GA', 'MS', 'TN'],
    'AK': [],
    'AZ': ['CA', 'CO', 'NM', 'NV', 'UT'],
    'AR': ['LA', 'MO', 'MS', 'OK', 'TN', 'TX'],
    'CA': ['AZ', 'NV', 'OR'],
    'CO': ['AZ', 'KS', 'NE', 'NM', 'OK', 'UT', 'WY'],
    'CT': ['MA', 'NY', 'RI'],
    'DE': ['MD', 'NJ', 'PA'],
    'FL': ['AL', 'GA'],
    'GA': ['AL', 'FL', 'NC', 'SC', 'TN'],
    'HI': [],
    'ID': ['MT', 'NV', 'OR', 'UT', 'WA', 'WY'],
    'IL': ['IA', 'IN', 'KY', 'MO', 'WI'],
    'IN': ['IL', 'KY', 'MI', 'OH'],
    'IA': ['IL', 'MN', 'MO', 'NE', 'SD', 'WI'],
    'KS': ['CO', 'MO', 'NE', 'OK'],
    'KY': ['IL', 'IN', 'MO', 'OH', 'TN', 'VA', 'WV'],
    'LA': ['AR', 'MS', 'TX'],
    'ME': ['NH'],
    'MD': ['DE', 'PA', 'VA', 'WV'],
    'MA': ['CT', 'NH', 'NY', 'RI', 'VT'],
    'MI': ['IN', 'OH', 'WI'],
    'MN': ['IA', 'ND', 'SD', 'WI'],
    'MS': ['AL', 'AR', 'LA', 'TN'],
    'MO': ['AR', 'IA', 'IL', 'KS', 'KY', 'NE', 'OK', 'TN'],
    'MT': ['ID', 'ND', 'SD', 'WY'],
    'NE': ['CO', 'IA', 'KS', 'MO', 'SD', 'WY'],
    'NV': ['AZ', 'CA', 'ID', 'OR', 'UT'],
    'NH': ['MA', 'ME', 'VT'],
    'NJ': ['DE', 'NY', 'PA'],
    'NM': ['AZ', 'CO', 'OK', 'TX', 'UT'],
    'NY': ['CT', 'MA', 'NJ', 'PA', 'VT'],
    'NC': ['GA', 'SC', 'TN', 'VA'],
    'ND': ['MN', 'MT', 'SD'],
    'OH': ['IN', 'KY', 'MI', 'PA', 'WV'],
    'OK': ['AR', 'CO', 'KS', 'MO', 'NM', 'TX'],
    'OR': ['CA', 'ID', 'NV', 'WA'],
    'PA': ['DE', 'MD', 'NJ', 'NY', 'OH', 'WV'],
    'RI': ['CT', 'MA'],
    'SC': ['GA', 'NC'],
    'SD': ['IA', 'MN', 'MT', 'ND', 'NE', 'WY'],
    'TN': ['AL', 'AR', 'GA', 'KY', 'MO', 'MS', 'NC', 'VA'],
    'TX': ['AR', 'LA', 'NM', 'OK'],
    'UT': ['AZ', 'CO', 'ID', 'NV', 'NM', 'WY'],
    'VT': ['MA', 'NH', 'NY'],
    'VA': ['KY', 'MD', 'NC', 'TN', 'WV'],
    'WA': ['ID', 'OR'],
    'WV': ['KY', 'MD', 'OH', 'PA', 'VA'],
    'WI': ['IA', 'IL', 'MI', 'MN'],
    'WY': ['CO', 'ID', 'MT', 'NE', 'SD', 'UT']
}

def haversine(lat1, lon1, lat2, lon2):

    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    angle = sin((lat2 - lat1)/2)**2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1)/2)**2
    dist = 2 * asin(sqrt(angle))
    return dist * 6371

def extract_2nd(a):
    return a[1]

def rank_by_location_with_embeddings(query, state_u, lat_u, lon_u, model, index, business_ids, business_info):
    allowed_states = [state_u] + bordering_states.get(state_u, [])

    # Encode query with SentenceTransformer
    query_vec = model.encode([query], convert_to_numpy=True)
    _, I = index.search(query_vec, 100)
    top_ids = [business_ids[i] for i in I[0]]

    restaurant_values_embed = []
    for bid in top_ids:
        info = business_info.get(bid, {})
        # Check if any info is empty
        lat, lon = info.get("latitude"), info.get("longitude")
        if not lat or not lon:
            continue
        if not info.get("reviews"):
            continue
        # Check if the state is close, if not skip
        if info.get("state") not in allowed_states:
            continue

        # Find distance
        distance = haversine(lat_u, lon_u, lat, lon)
        value = info["stars"]

        if distance > 5:
            # Only penalize the portion over 5km
            value = value - 0.005 * (distance - 5)

        restaurant_values_embed.append((bid, value))


    final_ranking = sorted(restaurant_values_embed, key=extract_2nd, reverse=True)
    lowest_score = final_ranking[-1][1]

    if lowest_score < 0:

        for i in range(len(final_ranking)):

            _, curr_score = final_ranking[i]
            final_ranking[i] = (_, curr_score - lowest_score)

    return final_ranking

# Query and rank

user_state = "TN"
user_lat = 36.1650
user_lon = -86.7840

'''results = rank_by_location_with_embeddings(
    "vegan, user_state, user_lat, user_lon,
    model, index, business_ids, business_information
)'''

ranked_results_location = rank_by_location_with_embeddings(
    query, user_state, user_lat, user_lon,
    model, index, business_ids, business_information
)

# Print top results
for bid, score in ranked_results_location:
    print(f"{business_information[bid]['name']} ({bid}): score = {score:.2f}")

"""## Tanmay's Part"""
print("TANMAY")
def apply_rating_penalty(avg_rating, threshold,curve="linear", param = 1):

    diff = max(0.0, threshold - avg_rating)
    #percentage difference
    scale = diff / threshold

    scale = scale * param

    if curve == "linear":
        return 1.0 - scale
    elif curve == "square":
        return 1.0 - scale**2
    elif curve == "exp":
        return np.exp(-5 * scale)
    else:
        return 1.0  # No penalty

def rank_with_similarity_and_rankingpenalty(similarities, business_ids, business_information, threshold=3.0, curve="linear",param = 1):
    ranked = []

    #for every business id
    for i, bid in enumerate(business_ids):

        avg_rating = business_information[bid].get("stars", 0.0)

        penalty_weight = apply_rating_penalty(avg_rating, threshold, curve,param)


        adjusted_score = similarities[i] * penalty_weight


        ranked.append((bid, adjusted_score))

    ranked.sort(key=lambda x: x[1], reverse=True)

    return ranked


def get_map_100(ranked_list, ground_truth, relevant_label="highly"):
    relevant_found = 0
    precision_total = 0.0

    for i, (bid, _) in enumerate(ranked_list[:100]):
        if ground_truth.get(bid) == relevant_label:
            relevant_found += 1
            precision_total += relevant_found / (i + 1)

    return precision_total / relevant_found if relevant_found > 0 else 0.0

##Finding Optimal Params

"""## Ground Truth (Praveen's Part)"""
print("GROUND TRUTH")
def build_ground_truth_level_B(biz_path, rev_path, query_keywords, location,
                               max_matches=5000, min_stars=0.0):
    biz_meta = {}
    with open(biz_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= max_matches: break
            b = json.loads(line)
            biz_meta[b["business_id"]] = b

    # load up to 10 reviews per business
    reviews = defaultdict(list)
    with open(rev_path, "r", encoding="utf-8") as f:
        for line in f:
            r = json.loads(line)
            bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid]) < 10:
                reviews[bid].append(r["text"].lower())

    gt = {}
    for bid, info in biz_meta.items():
        city = info.get("city","").lower().strip()
        cats = (info.get("categories") or "").lower()
        stars = float(info.get("stars",0))
        # metadata filters
        if city != location or stars < min_stars or not any(k in cats for k in ["cafe","coffee","espresso","tea"]):
            gt[bid] = "irrelevant"
        else:
            text = " ".join(reviews.get(bid, []))
            gt[bid] = "somewhat" if any(qk in text for qk in query_keywords) else "irrelevant"
    return gt, biz_meta

def build_ground_truth_level_B_all(biz_path, rev_path, query_keywords, min_stars=0.0):

    # Load all business metadata

    biz_meta = {}

    with open(biz_path, "r", encoding="utf-8") as f:

        for line in f:

            b = json.loads(line)

            biz_meta[b["business_id"]] = b

    # Load up to 10 reviews per business

    reviews = defaultdict(list)

    with open(rev_path, "r", encoding="utf-8") as f:

        for line in f:

            r = json.loads(line)

            bid = r["business_id"]

            if bid in biz_meta and len(reviews[bid]) < 10:

                reviews[bid].append(r["text"].lower())

    # Assign labels

    gt = {}

    for bid, info in biz_meta.items():

        cats = (info.get("categories") or "").lower()

        stars = float(info.get("stars", 0))

        # metadata filters: only check category and stars

        if stars < min_stars or not any(k in cats for k in ["cafe", "coffee", "espresso", "tea"]):

            gt[bid] = "irrelevant"

        else:

            text = " ".join(reviews.get(bid, []))

            gt[bid] = "somewhat" if any(qk in text for qk in query_keywords) else "irrelevant"

    return gt, biz_meta

biz_path = "yelp_academic_dataset_business.json"

rev_path = "yelp_academic_dataset_review.json"

query_keywords = ["music", "live", "bar", "band"]

gt, meta = build_ground_truth_level_B_all(biz_path, rev_path, query_keywords, min_stars=3.0)

# Save to disk

with open("ground_truth_level_B_full.json", "w") as f:

    json.dump(gt, f, indent=2)

import json
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

def build_ground_truth_level_A_plus_all(biz_path, rev_path, query, query_keywords,
                                        location, min_stars=3.0,
                                        thresholds=(0.5, 0.3), max_reviews=10):
    """
    """
    # Load business metadata
    biz_meta = {}
    with open(biz_path, "r", encoding="utf-8") as f:
        for line in f:
            b = json.loads(line)
            cats = (b.get("categories") or "").lower()
            stars = float(b.get("stars", 0))
            city = (b.get("city") or "").lower().strip()

            # Metadata filters
            #if location.lower() not in city:
            #    continue
            if stars < min_stars:
                continue
            if not any(k in cats for k in ["cafe", "coffee", "espresso", "tea"]):
                continue

            biz_meta[b["business_id"]] = b

    print(f"Businesses after filtering: {len(biz_meta)}")

    # Load up to N reviews per business
    reviews = defaultdict(list)
    with open(rev_path, "r", encoding="utf-8") as f:
        for line in f:
            r = json.loads(line)
            bid = r["business_id"]
            if bid in biz_meta and len(reviews[bid]) < max_reviews:
                reviews[bid].append(r["text"])

    print(f"Businesses with loaded reviews: {len(reviews)}")

    # Load SBERT model and encode query
    model = SentenceTransformer("all-MiniLM-L6-v2")
    q_emb = model.encode(query)

    # Assign labels
    gt = {}
    for bid, info in biz_meta.items():
        review_texts = reviews.get(bid, [])
        if not review_texts:
            gt[bid] = "irrelevant"
            continue

        combined_text = " ".join(review_texts).lower()
        keyword_flag = any(k in combined_text for k in query_keywords)

        doc_emb = model.encode(combined_text)
        sim = cosine_similarity([q_emb], [doc_emb]).flatten()[0]

        if sim >= thresholds[0]:
            gt[bid] = "highly"
        elif sim >= thresholds[1] or keyword_flag:
            gt[bid] = "somewhat"
        else:
            gt[bid] = "irrelevant"

    print(f"Final labeled businesses: {len(gt)}")
    return gt, biz_meta



gt, meta = build_ground_truth_level_A_plus_all(
    
    "yelp_academic_dataset_business.json",
    "yelp_academic_dataset_review.json",
    query=query,
    query_keywords=keywords,
    location=location,
    min_stars=3.0,
    thresholds=(0.4, 0.1)  # more forgiving if needed
)

# Get the business IDs actually used in your 10K review subset
sampled_bids = set(business_information.keys())

# Filter the ground truth and meta to include only those businesses
gt = {bid: label for bid, label in gt.items() if bid in sampled_bids}
meta = {bid: info for bid, info in meta.items() if bid in sampled_bids}

print(f"Filtered ground truth to {len(gt)} businesses in the sampled review set.")

# Save to file
with open("ground_truth_level_A_plus_all.json", "w") as f:
    json.dump({query: gt}, f, indent=2)

number_of_ids_found = 0
for id, _ in scores[:1000]:

    if id in meta: number_of_ids_found += 1

print(f"no. of id's found: {number_of_ids_found}")

def precision_at_k(ranked_list, ground_truth, k=10, relevant_label="highly"):
    hits = 0
    for i, (bid, _) in enumerate(ranked_list[:k]):
        if ground_truth.get(bid) == relevant_label:
            hits += 1
    return hits / k

def recall_at_k(ranked_list, ground_truth, k=100, relevant_label="highly"):
    relevant_total = sum(1 for v in ground_truth.values() if v == relevant_label)
    hits = 0
    for bid, _ in ranked_list[:k]:
        if ground_truth.get(bid) == relevant_label:
            hits += 1
    return hits / relevant_total if relevant_total > 0 else 0.0

# Baseline TF-IDF or SBERT similarity (no penalty)
ranked_results_tfidf = [(business_ids[i], similarities[i]) for i in range(len(business_ids))]
ranked_results_tfidf.sort(key=lambda x: x[1], reverse=True)

# Load ground truth if not yet loaded
with open("ground_truth_level_A_plus_all.json") as f:
    gt_data = json.load(f)
    # Use the specific query string as key
    gt = list(gt_data.values())[0]  # or gt_data["Cafe with great lattes"]

# Compute MAP@100 baseline
map_tfidf = get_map_100(ranked_results_tfidf, gt)
print(f"MAP@100 without rating penalty: {map_tfidf:.4f}")

"Graphs for the report"
# 1. Class distribution
labels = list(gt.values())
label_counts = Counter(labels)

print("\nLabel Distribution:")
for label, count in label_counts.items():
    print(f"{label}: {count}")

# Plot class distribution
plt.figure(figsize=(6, 4))
sns.barplot(x=list(label_counts.keys()), y=list(label_counts.values()))
plt.title("Ground Truth Label Distribution")
plt.ylabel("Number of Businesses")
plt.xlabel("Relevance Label")
plt.tight_layout()
plt.show()

# 2. Avg star rating by label
stars_by_label = defaultdict(list)
for bid, label in gt.items():
    stars = float(meta.get(bid, {}).get("stars", 0.0))
    stars_by_label[label].append(stars)

print("\nAverage Star Rating by Label:")
for label, stars in stars_by_label.items():
    avg = sum(stars) / len(stars)
    print(f"{label}: {avg:.2f}")

# Plot star rating by label
plt.figure(figsize=(6, 4))
sns.boxplot(data=[stars_by_label[l] for l in ['highly', 'somewhat', 'irrelevant']],
            palette='pastel')
plt.xticks([0, 1, 2], ['highly', 'somewhat', 'irrelevant'])
plt.ylabel("Star Rating")
plt.title("Star Ratings per Relevance Class")
plt.tight_layout()
plt.show()

print("TANMAY'S PART GRAPHS:PARAMETER TUNING")
curves = ["linear", "square", "exp"]
params = [0.2, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0]

best_map = 0.0
best_curve = None
best_param = None

results = []

for curve in curves:
    for param in params:
        ranked_results = rank_with_similarity_and_rankingpenalty(
            similarities,
            business_ids,
            business_information,
            threshold=3.0,
            curve=curve,
            param=param
        )

        map_score = get_map_100(ranked_results,
                                gt,
                                relevant_label="highly"
                                #need gt labels to make sure all results are checked properly
                                )

        results.append({
            "curve": curve,
            "param": param,
            "map@100": map_score
        })

        print(f"Curve: {curve}, Param: {param}, MAP@100: {map_score:.4f}")

        if map_score > best_map:
            best_map = map_score
            best_curve = curve
            best_param = param

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Print best config
print("\nBest configuration:")
print(f"Curve: {best_curve}")
print(f"Param: {best_param}")
print(f"MAP@100: {best_map:.4f}")

# Re-compute the best ranking using the selected best curve and param
ranked_results_penalty = rank_with_similarity_and_rankingpenalty(
    similarities,
    business_ids,
    business_information,
    threshold=3.0,
    curve=best_curve,
    param=best_param
)

prec10_tfidf = precision_at_k(ranked_results_tfidf, gt)
prec10_penalty = precision_at_k(ranked_results_penalty, gt)

print(f"Precision@10 without penalty: {prec10_tfidf:.4f}")
print(f"Precision@10 with penalty: {prec10_penalty:.4f}")

recall100_tfidf = recall_at_k(ranked_results_tfidf, gt)
recall100_penalty = recall_at_k(ranked_results_penalty, gt)

print(f"Recall@100 without penalty: {recall100_tfidf:.4f}")
print(f"Recall@100 with penalty: {recall100_penalty:.4f}")

ndcg_tfidf = ndcg_at_k(ranked_results_tfidf, gt, k=100)
ndcg_penalty = ndcg_at_k(ranked_results_penalty, gt, k=100)

print(f"NDCG@100 without penalty: {ndcg_tfidf:.4f}")
print(f"NDCG@100 with penalty: {ndcg_penalty:.4f}")

map_location = get_map_100(ranked_results_location, gt)
prec10_location = precision_at_k(ranked_results_location, gt)
recall100_location = recall_at_k(ranked_results_location, gt)
ndcg_location = ndcg_at_k(ranked_results_location, gt)

print(f"MAP@100 (Location-Aware): {map_location:.4f}")
print(f"Precision@10 (Location-Aware): {prec10_location:.4f}")
print(f"Recall@100 (Location-Aware): {recall100_location:.4f}")
print(f"NDCG@100 (Location-Aware): {ndcg_location:.4f}")

